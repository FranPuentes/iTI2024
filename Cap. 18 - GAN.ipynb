{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPsjiK86U93Aj7avRd8CJp5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<table align=\"left\">\n","  <td>\n","    <a href=\"https://colab.research.google.com/drive/1D62kStjydJvh9Xz4ePYRaqY_hgXMSG7J\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","  </td>\n","</table>"],"metadata":{"id":"JJHXhaeUndiQ"}},{"cell_type":"markdown","source":["# **Redes neuronales antagonistas**\n"],"metadata":{"id":"Kw3rbHlcXzEe"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","\n","from tensorflow.keras.datasets import fashion_mnist\n"],"metadata":{"id":"IdMqAYoyow9R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_generator():\n","\n","    model = Sequential()\n","    model.add(Dense(256, input_dim=100))\n","    model.add(LeakyReLU(0.2))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Dense(512))\n","    model.add(LeakyReLU(0.2))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Dense(1024))\n","    model.add(LeakyReLU(0.2))\n","    model.add(BatchNormalization(momentum=0.8))\n","    model.add(Dense(np.prod((28, 28, 1)), activation='tanh'))\n","    model.add(Reshape((28, 28, 1)))\n","\n","    noise = Input(shape=(100,))\n","    img = model(noise)\n","\n","    return Model(noise, img)\n","\n","def build_discriminator():\n","\n","    model = Sequential()\n","    model.add(Flatten(input_shape=(28, 28, 1)))\n","    model.add(Dense(512))\n","    model.add(LeakyReLU(0.2))\n","    model.add(Dense(256))\n","    model.add(LeakyReLU(0.2))\n","    model.add(Dense(1, activation='sigmoid'))\n","\n","    img = Input(shape=(28, 28, 1))\n","    validity = model(img)\n","\n","    return Model(img, validity)\n"],"metadata":{"id":"cGvddfvpo0O_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def compile_gan():\n","\n","    #optimizer = Adam(0.0002, 0.5)\n","    optimizer = tf.keras.optimizers.legacy.Adam(0.0002, 0.5)\n","\n","    # Construir y compilar el discriminador\n","    discriminator = build_discriminator()\n","    discriminator.compile(loss='binary_crossentropy',\n","                           optimizer=optimizer,\n","                           metrics=['accuracy'])\n","\n","    # Construir el generador\n","    generator = build_generator()\n","\n","    # Generador toma ruido como entrada y genera imágenes\n","    z = Input(shape=(100,))\n","    img = generator(z)\n","\n","    # Solo entrenaremos el generador en el modelo combinado\n","    discriminator.trainable = False\n","\n","    # Discriminador toma imágenes generadas como entrada y determina validez\n","    validity = discriminator(img)\n","\n","    # Modelo combinado (stacked generator and discriminator)\n","    combined = Model(z, validity)\n","    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","    return generator, discriminator, combined\n"],"metadata":{"id":"JO2kYfWNo67u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_gan(epochs, batch_size=128, save_interval=50):\n","\n","    (X_train, _), (_, _) = fashion_mnist.load_data()\n","\n","    X_train = X_train / 127.5 - 1.\n","    X_train = np.expand_dims(X_train, axis=3)\n","\n","    valid = np.ones((batch_size, 1))\n","    fake = np.zeros((batch_size, 1))\n","\n","    generator, discriminator, combined = compile_gan()\n","\n","    for epoch in range(epochs):\n","\n","        # Entrenar el discriminador\n","        idx = np.random.randint(0, X_train.shape[0], batch_size)\n","        imgs = X_train[idx]\n","\n","        noise = np.random.normal(0, 1, (batch_size, 100))\n","        gen_imgs = generator.predict(noise)\n","\n","        d_loss_real = discriminator.train_on_batch(imgs, valid)\n","        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n","        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","        # Entrenar el generador\n","        noise = np.random.normal(0, 1, (batch_size, 100))\n","        g_loss = combined.train_on_batch(noise, valid)\n","\n","        print(f\"{epoch}/{epochs} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")\n","\n","        if epoch % save_interval == 0:\n","            save_imgs(epoch, generator)\n","\n","def save_imgs(epoch, generator):\n","    r, c = 5, 5\n","    noise = np.random.normal(0, 1, (r * c, 100))\n","    gen_imgs = generator.predict(noise)\n","\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","        for j in range(c):\n","            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","            axs[i,j].axis('off')\n","            cnt += 1\n","    plt.show()\n"],"metadata":{"id":"DjY8RIQ_o-MY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_gan(epochs=10000, batch_size=32, save_interval=1000)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1XW2K4pHvGqy9REYL8Kgh8j-R0WY_Vj9z"},"id":"I4Ek_w4WpAXt","executionInfo":{"status":"ok","timestamp":1695400157883,"user_tz":-120,"elapsed":1211292,"user":{"displayName":"Juan Francisco Puentes Calvo","userId":"04307711343502565257"}},"outputId":"4b6e68be-229a-446f-aa4d-b8a9e879b8db"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["def generate_and_show_imgs(generator, n_images=25):\n","    r, c = 5, 5  # Esto generará una cuadrícula de 5x5 de imágenes. Asegúrate de que r*c = n_images\n","    if n_images != r*c:\n","        raise ValueError(\"r*c must be equal to n_images\")\n","\n","    noise = np.random.normal(0, 1, (r * c, 100))\n","    gen_imgs = generator.predict(noise)\n","\n","    # Re-escala imágenes 0 - 1\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    fig, axs = plt.subplots(r, c, figsize=(10, 10))\n","    cnt = 0\n","    for i in range(r):\n","        for j in range(c):\n","            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","            axs[i,j].axis('off')\n","            cnt += 1\n","    plt.show()\n"],"metadata":{"id":"fpb-RifFpDLn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator, _, _ = compile_gan()  # Solo necesitas el generador aquí, por eso ignoramos el discriminador y el modelo combinado.\n","train_gan(epochs=10000, batch_size=32, save_interval=1000)\n","\n","# Genera y muestra 25 imágenes\n","generate_and_show_imgs(generator)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1XDnspzKzU_DV4Jg6dVO_snST9eEzH4cj"},"id":"neEsLkE8p_nL","executionInfo":{"status":"ok","timestamp":1695401457899,"user_tz":-120,"elapsed":1237551,"user":{"displayName":"Juan Francisco Puentes Calvo","userId":"04307711343502565257"}},"outputId":"e11a4806-7581-4495-dfb6-5797dee1257d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}